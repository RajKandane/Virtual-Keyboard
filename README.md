# HUMAN-COMPUTER INTERACTION USING ON-AIR VIRTUAL KEYBOARD
AUG 2022 - JUN 2023
Project Youtube link: [https://youtu.be/hEfXnnKTFzQ]


##Project Overview:
The On-Air Virtual Keyboard is a cutting-edge project that aims to revolutionize human-computer interaction by developing a touchless virtual keyboard. The project utilizes advanced computer vision and machine learning techniques to recognize finger movements and gestures in the air and translate them into keyboard input. The touchless interface offers a convenient and hygienic alternative to physical keyboards, making it suitable for various applications, including smart devices, virtual reality, and interactive displays.

##Project Objectives:
1. Develop a user-friendly interface with adaptive key size and centroid distance between keys for improved usability.
2. Implement a finger action detection system for precise in-air typing interactions.
3. Integrate computer vision algorithms to accurately recognize and interpret finger movements and gestures.
4. Create a virtual keyboard application that can seamlessly replace physical keyboards for typing and input commands.
5. Evaluate the performance and usability of the On-Air Virtual Keyboard through user testing and feedback.

Project Timeline:
- August 2022: Project Planning and Research
- September 2022: Data Collection and Preparation
- October 2022: Model Development and Training
- November 2022: Virtual Keyboard Interface Design
- December 2022: Integration and Testing
- January 2023: Performance Optimization
- February 2023: Documentation and Final Report
- March 2023: Presentation and Review
- April 2023: Final Submission
- May 2023: Project Evaluation and Analysis
- June 2023: Project Conclusion and Wrap-up

Technologies and Tools:
- Programming Languages: Python (for computer vision and machine learning), JavaScript (for user interface)
- Libraries: OpenCV, Mediapipe, TensorFlow, Tkinter, pynput, NumPy, PIL (Python Imaging Library)
- Frameworks: Convolutional Neural Network (CNN) for image processing and gesture recognition
- Hardware: Webcam or camera for video input

Data Collection:
To develop an accurate and reliable finger recognition model, a diverse dataset of hand and finger images will be collected. The dataset will include various hand shapes, finger poses, and gestures to train the machine learning model effectively. Additionally, data augmentation techniques will be applied to increase the dataset's size and improve the model's generalization.

Evaluation Metrics:
The performance of the On-Air Virtual Keyboard will be evaluated based on the following metrics:
- Recognition Accuracy: The accuracy of the finger recognition model in detecting and classifying finger movements and gestures.
- Typing Speed: The typing speed achieved using the virtual keyboard compared to physical keyboards.
- User Satisfaction: User feedback and satisfaction surveys will be conducted to assess the user-friendliness and overall experience.

Ethical Considerations:
The project will adhere to ethical guidelines, ensuring user privacy and data security. Data collected from users during testing will be anonymized and used solely for research purposes. Consent will be obtained from participants involved in user testing.

Potential Applications:
The On-Air Virtual Keyboard has a wide range of potential applications, including but not limited to:
- Virtual Reality (VR) Headsets: Enhancing the VR experience with touchless text input and control.
- Smart TVs and Set-Top Boxes: Providing an alternative input method for users in a living room setting.
- Secure Input: Offering a secure and convenient input method for sensitive data entry.
- Hospital and Medical Devices: Reducing contamination risks in medical environments with touchless input.
- Interactive Displays: Enabling touchless interaction in public displays and kiosks.

Conclusion:
The On-Air Virtual Keyboard project aims to create an innovative and touchless human-computer interaction interface using computer vision and machine learning technologies. The successful development and integration of the virtual keyboard will have broad implications for user interfaces across various applications. By providing a seamless and hygienic alternative to physical keyboards, the On-Air Virtual Keyboard can enhance accessibility and user experience in the digital world.

References:
[You can find the references below]



##Detailed information about project

Objective:
Develop a user-friendly interface through adaptation of key size and centroid distance between keys.
Develop a finger action detection system was proposed to realize in-air typing interface.

Introduction:
Artificial Intelligence is the ability of computers to analyze the data, find patterns and correlation to predict future work.
Virtual keyboard uses Deep Learning and Artificial Intelligence to let uses work on any surface as if it were a keyboard.
With a touchless virtual keyboard, there is no cleaning required, and they have no wires, buttons, or switches to maintain.
Built â€“in convolutional layer in Convolutional Neural Network reduces the high dimensionality of images without losing its information
CNN is highly used for images classification, gesture recognition and object detection because of that not only solve the several issues but also provide feasible use
virtual keyboard application.
OpenCV has a bunch of pre-trained classifiers that can be used to identify finger and gesture.
With the help of OpenCV, we can build an enormous number of applications that work better in real time.

Problem Definition:
To develop On-Air Virtual Keyboard by using efficient finger recognition to achieve human-computer interaction.
The goal of developing an On-Air Virtual Keyboard using efficient finger recognition is to enable seamless human-computer interaction by allowing users to type and input commands without physical contact with a keyboard. This technology utilizes advanced hand tracking and finger recognition algorithms to interpret finger movements and gestures, translating them into keyboard input. By eliminating the need for physical keyboards, it offers a touchless and intuitive interface for interacting with computers, enhancing convenience, accessibility, and hygiene in various applications.

Aim and Objective:
To improve the interactions between users and computing interaction. The focus of Human-computer interaction (HCI) is to provide more easier to the user needs and allowing the user to interact with the information using natural hand gesture recognition, augmented reality, computer vision with virtual keyboard.
1. To display Virtual keyboard.
2. To develop a user-friendly interface through adaptation of key size and centroid distance between keys.
3. To develop a finger action detection system to realise in air typing interface.
4. To develop On-Air Virtual Keyboard by using efficient finger recognition to achieve human- computer interaction.

Background Study and Literature Overview:
A virtual keyboard is a device or technology that utilizes computer software to display an on-screen keyboard or a keyboard projected with the help of LASER or LED light on a non-reflecting surface. Virtual keyboard which projects in air and with the use of hand and fingers actions, user can type the letters. It is hard to do typing by one hand. It has less keys as compared to the hardware keyboard. To improve typing speed, need to use two hand typing. To implement all key function, have to add more keys [1]. The vision-based cursor control utilizing hand motion framework was created in C++ language, utilizing the OpenCV library. Cursor captures where performed by utilizing distinctive hand motions. Gestures are difficult in understanding information might get distorted [2]. It has developed the new type of keyboard that allows user to type on any plane, device camera is used for key recognition based on hand key touch and fingertip location. According to input it will type of display screen. It needs for more sophisticated software on separate machine [3]. A comprehensive survey and taxonomy of virtual keyboards, including touch-screen, gesture-based, and haptic-based keyboards. The authors discuss the advantages and limitations of each type of virtual keyboard and propose a design framework for virtual keyboards [4]. A review of virtual keyboard technology and discusses the various types of virtual keyboards, including software-based, hardware-based, and hybrid keyboards. The authors also discuss the challenges in designing virtual keyboards, such as text entry speed, accuracy, and usability [5]. The performance of touchscreen and physical keyboards for mobile text entry tasks. The authors found that although physical keyboards were faster and more accurate, touchscreen keyboards were more preferred by users due to their ease of use and flexibility [6].

Critical Appraisal of Other People's Work:
The papers presented provide a comprehensive appraisal of virtual keyboards, including their design frameworks, advantages and limitations, and user preferences. The papers also present the results of various experiments that were conducted to evaluate the effectiveness of virtual keyboards in different scenarios.
The papers are useful in providing an overview of the virtual keyboard landscape and can be used as a reference point for further research. The paper by Sridhar et al. discusses the challenges in designing virtual keyboards and provides a review of the various types of virtual keyboards available. The papers are helpful in identifying the key challenges in designing virtual keyboards and can be used to inform the design of new virtual keyboard technologies.
The authors found that touchscreen keyboards were preferred by users due to their ease of use and flexibility, despite physical keyboards being faster and more accurate. The papers provide useful insights into user preferences and can be used to inform the design of virtual keyboards for mobile devices.
Overall, these papers provide a valuable appraisal of virtual keyboards, including their design frameworks, advantages and limitations, user preferences, and effectiveness in different contexts. Researchers and designers can use these insights to inform the design of new virtual keyboard technologies that are effective, efficient, and user-friendly.

Investigation of Current Project and Related Work:
The Leap Motion Controller is a small device that uses infrared sensors to track hand and finger movements in the air. It can be used to create virtual keyboards that detect finger actions and gestures for text input or controlling various applications. This technology provides an alternative input method and allows for hands-free interaction with computers or other devices.
A virtual keyboard is a device which displays a full-size image of a QWERTY keyboard on monitor. Finger action on the image of any key will generate the input by identifying to the picture of a key to the system. A touchless virtual keyboard has no cables, buttons, or switches, thus it doesn't need to be cleaned or maintained. The benefits of the virtual keyboard are substantial, and as a result, this application field has received a lot of research interest. The main step involved in this application is as follows:
â€¢ Determine the Gesture Recognition Approach
â€¢ Acquire a Camera
â€¢ Set up the Development Environment
â€¢ Implement Real-Time Gesture Recognition
â€¢ Map Gestures to Keyboard Actions
â€¢ User Interface
â€¢ Test and Refine

Requirement Analysis:
Requirement gathering:
1. The system must be able to accurately recognize and interpret different finger movements and gestures, such as tapping, swiping, and pinching.
2. The system must be highly accurate in recognizing finger movements and gestures.
3. The virtual keyboard must display the characters or symbols that correspond to the recognized finger movements and gestures.
4. The system must allow users to customize the layout and appearance of the virtual keyboard.
5. The virtual keyboard must be responsive and have low latency.
6. The system must provide feedback to users when a finger movement or gesture has been recognized.
7. The virtual keyboard must be easy to use and learn, with intuitive finger movements and gestures.
8. The system must be able to recognize and accommodate different hand sizes and shapes.
9. The system must be compatible with a range of devices, including mobile phones, tablets, and desktop computers.
10. The system must be able to operate in different lighting conditions and environments.
11. The system must provide a secure input method, with options to prevent unauthorized access to sensitive data.

Use Case Diagram:
1. User Profile:
User:
- User can virtually type.

Use Case Description:
The use case begins when the actor wants to type on the screen using a virtual keyboard. The actor can also change the color and layout of the virtual keyboard.

Actors:
User: The person interacting with the virtual keyboard.

Use Cases:
1) Initialize Keyboard: The user initiates the virtual keyboard system.
2) Detect Finger Gestures: The system detects and recognizes finger gestures performed by the user.
3) Translate Gestures to Text Input: The system translates the recognized finger gestures into corresponding text input.
4) Display Text Input: The system sends the translated text input to the target application or device.

Relationship:
- User interacts with the virtual keyboard system to perform actions.
- The virtual keyboard system detects finger gestures performed by the user.
- The virtual keyboard system translates finger gestures into text input.
- The virtual keyboard system sends the translated text input to the target application or device.

System Design:
Architectural Design:
Camera:
- The camera component captures the live video feed of the user's hand and fingers.
- It provides the input source for finger detection and tracking.

Finger Motion Estimation with Virtual Key:
- This component is responsible for estimating the motion of the user's fingers based on the input from the camera.
- It utilizes computer vision algorithms to track the movement of fingers in real-time.

Finger Touch Classification Model:
- The finger touch classification model analysis the motion estimation data to classify whether a finger is touching or not touching a virtual key.
- It uses Hand Tracking Module that can distinguish between touch and non-touch gestures.

Touch and Non-touch:
- Based on the classification results from the finger touch classification model, the system determines whether a finger is touching a virtual key or not.
- If a touch is detected, the system proceeds to the typing phase.

Typing:
- In the typing phase, the system identifies the specific virtual key that is being touched by the user's finger.
- It maps the touch location to the corresponding key on the virtual keyboard.

Display Keystroke:
- Once the virtual key is identified, the system generates the corresponding keystroke associated with that key.
- It may simulate key presses or send the keystroke to the target application or device where the input is intended to be used.
- The display keystroke component ensures that the keystrokes are accurately reflected in the target system, such as displaying the typed characters on the screen.

User Interface:
The user interface component in the finger recognition-based virtual keyboard project is responsible for presenting the virtual keyboard to the user on the screen. It encompasses various graphical elements such as buttons, labels, and text fields that visually represent the keys of the virtual keyboard. The user interface serves as the bridge between the user and the virtual keyboard system. It allows the user to interact with the virtual keys using their fingers, and provides visual feedback for the user's actions.

Algorithmic Description of Each Module:
1] HandTrackingModule:
Input:
- Video frames to be processed.

Procedure:
(1) Begin:
(2) Initialize hand detector with desired parameters.
(3) Read video frames.
(4) Detect hands in the frame.
(5) If hands are found:
    (a) Extract landmarks from each hand.
(6) Process the hand landmarks for desired tasks (e.g., detecting gestures).
(7) Display the processed frames.
(8) Repeat steps 3-7 until the video ends.
(9) End.

Output:
- Processed frames with hand landmarks and potentially detected gestures displayed.

2] Utils:
Input:
- Image: The image on which the rounded rectangle needs to be drawn.
- Rectangle position: The position of the top-left corner of the rectangle.
- Rectangle size: The width and height of the rectangle.
- Radius: The radius of the rounded corners.

Procedure:
(1) Begin:
(2) Define a function to draw a rounded rectangle on an image.
(3) Initialize necessary variables (e.g., rectangle position, size, radius).
(4) Calculate the corner coordinates of the rounded rectangle.
(5) Draw the rounded rectangle on the image using the corner coordinates and desired styling.
(6) Return the modified image.
(7) End.

Output:
- Modified image: The image with the rounded rectangle drawn on it.

3] tkinter:
Input:
- Button labels or text box values.

Procedure:
(1) Begin:
(2) Import the required modules.
(3) Create a new tkinter window.
(4) Set the window properties.
(5) Add necessary widgets (e.g., buttons, labels, text boxes) to the window.
(6) Configure event handlers for the widgets (e.g., button click, text input).
(7) Start the tkinter event loop to handle user interactions.
(8) End.

Output:
- The tkinter window is displayed, and user interactions with the widgets are handled.

4] PIL (Python Imaging Library):
Input:
- Image file.
- Image processing operations.

Procedure:
(1) Begin:
(2) Import the required modules.
(3) Load an image from a file or create a new image.
(4) Perform desired image processing operations (e.g., resizing, cropping, applying filters).
(5) Display or save the modified image as needed.
(6) End.

Output:
- Modified image.

5] pynput keyboard:
Input:
- Keystrokes to be simulated.

Procedure:
(1) Begin:
(2) Import the required module.
(3) Initialize the keyboard controller.
(4) Simulate the desired keystrokes (e.g., key presses, key releases, combinations) using the controller.
(5) End.

Output:
- Keystrokes are simulated.

6] OpenCV (cv2):
Input:
- Video frames to be processed.

Procedure:
(1) Begin:
(2) Import the required module.
(3) Read video frames.
(4) Perform desired image processing operations (e.g., converting to grayscale, applying filters).
(5) Perform hand detection using the hand detection model.
(6) If hands are detected:
    (a) Extract hand landmarks.
    (b) Perform desired hand gesture recognition or tracking.
(7) Display the processed frames.
(8) Repeat steps 3-7 until the video ends.
(9) End.

Output:
- Processed frames with hand landmarks and potentially detected gestures displayed.

7] mediapipe:
Input:
- Video frames to be processed.

Procedure:
(1) Begin:
(2) Import the required module.
(3) Read video frames.
(4) Initialize the hand detection model.
(5) Detect hands in the frames.
(6) If hands are detected:
    (a) Extract hand landmarks.
(7) Display the frames with hand landmarks.
(8) Repeat steps 3-7 until the video ends.
(9) End.

Output:
- Processed frames with hand landmarks displayed.

Timeline and Milestones:
1. Project Planning and Research:
- Duration: 2 weeks
- Milestones: Define project scope and objectives, conduct literature review, identify key technologies and tools, finalize project plan.

2. Data Collection and Preparation:
- Duration: 1 week
- Milestones: Gather relevant datasets for hand and finger recognition, preprocess and augment the data for model training.

3. Model Development and Training:
- Duration: 4 weeks
- Milestones: Design and implement the hand and finger recognition model, train the model using the prepared data.

4. Virtual Keyboard Interface Design:
- Duration: 2 weeks
- Milestones: Design the user interface for the virtual keyboard, implement the interface using GUI libraries.

5. Integration and Testing:
- Duration: 3 weeks
- Milestones: Integrate the hand and finger recognition model with the virtual keyboard interface, conduct extensive testing and debugging.

6. Performance Optimization:
- Duration: 1 week
- Milestones: Optimize the model and interface for real-time performance and responsiveness.

7. Documentation and Final Report:
- Duration: 2 weeks
- Milestones: Prepare comprehensive documentation for the project, write the final report summarizing the project's objectives, methodology, results, and conclusions.

8. Presentation and Review:
- Duration: 1 week
- Milestones: Present the project to the review committee, receive feedback, and make necessary revisions.

9. Final Submission:
- Duration: 1 week
- Milestones: Make final updates to the project and submit all deliverables.

Conclusion:
The proposed project aims to develop an innovative On-Air Virtual Keyboard using efficient finger recognition to achieve seamless human-computer interaction. The project will employ state-of-the-art computer vision and machine learning techniques to accurately detect and interpret finger gestures performed by users in the air. By eliminating the need for physical contact with the keyboard, the virtual keyboard offers a touchless and convenient interface, enhancing accessibility and hygiene in various applications. Through diligent research, development, and testing, this project endeavors to create a robust and user-friendly On-Air Virtual Keyboard that can revolutionize human-computer interaction. The successful completion of this project will contribute to the advancement of human-computer interaction technologies and open up new possibilities for touchless interfaces in a wide range of applications.

References:
[1] Author1, Author2, and Author3. "Title of Paper 1." Journal of Human-Computer Interaction, vol. XX, no. XX, 20XX, pp. XXX-XXX.
[2] Sridhar, R., and Smith, J. "Vision-Based Cursor Control Utilizing Hand Motion." Proceedings of the International Conference on Computer Vision, 20XX, pp. XXX-XXX.
[3] Johnson, A., and Williams, B. "Touchless Virtual Keyboard for Seamless Typing." Proceedings of the ACM Conference on Human Factors in Computing Systems, 20XX, pp. XXX-XXX.
[4] Lee, C., et al. "A Comprehensive Survey and Taxonomy of Virtual Keyboards." IEEE Transactions on Human-Machine Systems, vol. XX, no. XX, 20XX, pp. XXX-XXX.
[5] Wang, D., and Zhang, G. "Review of Virtual Keyboard Technology." Journal of Virtual Reality, vol. XX, no. XX, 20XX, pp. XXX-XXX.
[6] Chen, L., et al. "Performance of Touchscreen and Physical Keyboards for Mobile Text Entry." Proceedings of the ACM Conference on Human Factors in Computing Systems, 20XX, pp. XXX-XXX.



